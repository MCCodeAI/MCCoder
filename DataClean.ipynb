{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Code Dataset Clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been updated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'docs/WMX3API c++ to python list -all2.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to remove trailing semicolon or semicolon followed by a space\n",
    "def remove_trailing_semicolon(s):\n",
    "    if isinstance(s, str):\n",
    "        if s.endswith(';'):\n",
    "            return s[:-1]\n",
    "        elif s.endswith('; '):\n",
    "            return s[:-2]\n",
    "        elif s.endswith(';Â '):\n",
    "            return s[:-2]\n",
    "    return s\n",
    "\n",
    "# Apply the function to the 'FunctionC++' column\n",
    "df['FunctionC++'] = df['FunctionC++'].apply(remove_trailing_semicolon)\n",
    "\n",
    "# Save the updated DataFrame back to an Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"File has been updated and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been updated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'docs/table-data(Api)2.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to remove blank spaces in front of \"(\"\n",
    "def remove_blank_space(s):\n",
    "    if isinstance(s, str):\n",
    "        return s.replace(\" (\", \"(\")\n",
    "    return s\n",
    "\n",
    "# Apply the function to the 'APINAME' column\n",
    "df['APINAME'] = df['APINAME'].apply(remove_blank_space)\n",
    "\n",
    "# Save the updated DataFrame back to an Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"File has been updated and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been updated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'docs/WMX3API c++ to python list -all2.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to adjust spaces around '*'\n",
    "def adjust_asterisk_spacing(s):\n",
    "    if isinstance(s, str):\n",
    "        # Replace * without leading space and with trailing space\n",
    "        s = s.replace('* ', ' *')\n",
    "        # Replace * without leading space and with trailing space (fix any extra spaces)\n",
    "        s = s.replace('  *', ' *')\n",
    "         #' = '\n",
    "        s = s.replace(' = ', '=')\n",
    "       \n",
    "    return s\n",
    "\n",
    "# Apply the function to the 'FunctionC++' column\n",
    "df['FunctionC++'] = df['FunctionC++'].apply(adjust_asterisk_spacing)\n",
    "\n",
    "# Save the updated DataFrame back to an Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(\"File has been updated and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statictics of sample codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 1_MCEval.py, Tokens: 225\n",
      "File: 2_MCEval.py, Tokens: 250\n",
      "File: 3_MCEval.py, Tokens: 401\n",
      "File: 4_MCEval.py, Tokens: 433\n",
      "File: 5_MCEval.py, Tokens: 703\n",
      "File: 6_MCEval.py, Tokens: 217\n",
      "File: 7_MCEval.py, Tokens: 365\n",
      "File: 8_MCEval.py, Tokens: 368\n",
      "File: 9_MCEval.py, Tokens: 517\n",
      "File: 10_MCEval.py, Tokens: 437\n",
      "File: 11_MCEval.py, Tokens: 927\n",
      "File: 12_MCEval.py, Tokens: 741\n",
      "File: 13_MCEval.py, Tokens: 1220\n",
      "File: 14_MCEval.py, Tokens: 412\n",
      "File: 15_MCEval.py, Tokens: 425\n",
      "File: 16_MCEval.py, Tokens: 530\n",
      "File: 17_MCEval.py, Tokens: 41\n",
      "File: 18_MCEval.py, Tokens: 520\n",
      "File: 19_MCEval.py, Tokens: 482\n",
      "File: 20_MCEval.py, Tokens: 380\n",
      "File: 21_MCEval.py, Tokens: 368\n",
      "File: 22_MCEval.py, Tokens: 441\n",
      "File: 23_MCEval.py, Tokens: 504\n",
      "File: 24_MCEval.py, Tokens: 505\n",
      "File: 25_MCEval.py, Tokens: 539\n",
      "File: 26_MCEval.py, Tokens: 1055\n",
      "File: 27_MCEval.py, Tokens: 474\n",
      "File: 28_MCEval.py, Tokens: 788\n",
      "File: 29_MCEval.py, Tokens: 874\n",
      "File: 30_MCEval.py, Tokens: 904\n",
      "File: 31_MCEval.py, Tokens: 431\n",
      "File: 32_MCEval.py, Tokens: 775\n",
      "File: 33_MCEval.py, Tokens: 384\n",
      "File: 34_MCEval.py, Tokens: 387\n",
      "File: 35_MCEval.py, Tokens: 402\n",
      "File: 36_MCEval.py, Tokens: 813\n",
      "File: 37_MCEval.py, Tokens: 859\n",
      "File: 38_MCEval.py, Tokens: 1016\n",
      "File: 39_MCEval.py, Tokens: 982\n",
      "File: 40_MCEval.py, Tokens: 1358\n",
      "File: 41_MCEval.py, Tokens: 1443\n",
      "File: 42_MCEval.py, Tokens: 789\n",
      "File: 43_MCEval.py, Tokens: 2380\n",
      "File: 44_MCEval.py, Tokens: 1480\n",
      "File: 45_MCEval.py, Tokens: 1446\n",
      "File: 46_MCEval.py, Tokens: 1466\n",
      "File: 47_MCEval.py, Tokens: 1582\n",
      "File: 48_MCEval.py, Tokens: 1736\n",
      "File: 49_MCEval.py, Tokens: 1671\n",
      "File: 50_MCEval.py, Tokens: 1807\n",
      "File: 51_MCEval.py, Tokens: 1115\n",
      "File: 52_MCEval.py, Tokens: 1291\n",
      "File: 53_MCEval.py, Tokens: 1144\n",
      "File: 54_MCEval.py, Tokens: 1205\n",
      "File: 55_MCEval.py, Tokens: 1242\n",
      "File: 56_MCEval.py, Tokens: 1282\n",
      "File: 57_MCEval.py, Tokens: 939\n",
      "File: 58_MCEval.py, Tokens: 930\n",
      "File: 59_MCEval.py, Tokens: 1320\n",
      "File: 60_MCEval.py, Tokens: 1357\n",
      "File: 61_MCEval.py, Tokens: 1120\n",
      "File: 62_MCEval.py, Tokens: 1150\n",
      "File: 63_MCEval.py, Tokens: 1645\n",
      "File: 64_MCEval.py, Tokens: 952\n",
      "File: 65_MCEval.py, Tokens: 1079\n",
      "File: 66_MCEval.py, Tokens: 1079\n",
      "File: 67_MCEval.py, Tokens: 1294\n",
      "File: 68_MCEval.py, Tokens: 1188\n",
      "File: 69_MCEval.py, Tokens: 1369\n",
      "File: 70_MCEval.py, Tokens: 1065\n",
      "File: 71_MCEval.py, Tokens: 1095\n",
      "File: 72_MCEval.py, Tokens: 1107\n",
      "File: 73_MCEval.py, Tokens: 1020\n",
      "File: 74_MCEval.py, Tokens: 1053\n",
      "File: 75_MCEval.py, Tokens: 1008\n",
      "File: 76_MCEval.py, Tokens: 821\n",
      "File: 77_MCEval.py, Tokens: 609\n",
      "File: 78_MCEval.py, Tokens: 558\n",
      "File: 79_MCEval.py, Tokens: 558\n",
      "File: 80_MCEval.py, Tokens: 1020\n",
      "File: 81_MCEval.py, Tokens: 772\n",
      "File: 82_MCEval.py, Tokens: 1013\n",
      "File: 83_MCEval.py, Tokens: 775\n",
      "File: 84_MCEval.py, Tokens: 798\n",
      "File: 85_MCEval.py, Tokens: 1401\n",
      "File: 86_MCEval.py, Tokens: 1391\n",
      "File: 87_MCEval.py, Tokens: 2104\n",
      "File: 88_MCEval.py, Tokens: 9313\n",
      "File: 89_MCEval.py, Tokens: 6887\n",
      "File: 90_MCEval.py, Tokens: 4233\n",
      "File: 91_MCEval.py, Tokens: 2568\n",
      "File: 92_MCEval.py, Tokens: 1890\n",
      "File: 93_MCEval.py, Tokens: 1186\n",
      "File: 94_MCEval.py, Tokens: 3228\n",
      "File: 95_MCEval.py, Tokens: 3954\n",
      "File: 96_MCEval.py, Tokens: 1219\n",
      "File: 97_MCEval.py, Tokens: 22431\n",
      "File: 98_MCEval.py, Tokens: 2597\n",
      "Average tokens length of all files: 1424.78\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# Define the folder path and initialize variables\n",
    "folder_canonical = '/Users/yin/Documents/GitHub/MCCoder/MCEval_Files/Sample codes'\n",
    "total_tokens = 0\n",
    "file_count = 0\n",
    "\n",
    "# Define a function to read and tokenize a file\n",
    "def get_token_length(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file and returns the number of tokens.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(content)\n",
    "    return len(tokens)\n",
    "\n",
    "# Iterate through task_ids from 1 to 98\n",
    "for task_id in range(1, 99):\n",
    "    filename = f'{task_id}_MCEval.py'\n",
    "    file_path = os.path.join(folder_canonical, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        tokens_length = get_token_length(file_path)\n",
    "        total_tokens += tokens_length\n",
    "        file_count += 1\n",
    "        print(f'File: {filename}, Tokens: {tokens_length}')\n",
    "    else:\n",
    "        print(f'File not found: {filename}')\n",
    "\n",
    "# Calculate the average token length\n",
    "if file_count > 0:\n",
    "    average_tokens = total_tokens / file_count\n",
    "    print(f'Average tokens length of all files: {average_tokens:.2f}')\n",
    "else:\n",
    "    print('No files found.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statictics of Cannoical codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Instruction Length: 272.88793103448273\n",
      "Average Instruction Length by Difficulty: {1: 266.655737704918, 2: 189.25, 3: 405.7826086956522}\n",
      "Overall Average Canonical Code Length: 2145.543103448276\n",
      "Average Canonical Code Length by Difficulty: {1: 2268.0, 2: 1351.09375, 3: 2926.086956521739}\n",
      "Average Instruction Length for Difficulty 1: 266.655737704918\n",
      "Average Instruction Length for Difficulty 2: 189.25\n",
      "Average Instruction Length for Difficulty 3: 405.7826086956522\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Load the dataset\n",
    "folder_dataset = '/Users/yin/Documents/GitHub/MCCoder/docs/WMX3API_MCEval_Evaluation_Dataset.json'\n",
    "folder_canonical = '/Users/yin/Documents/GitHub/MCCodeLog/CanonicalCode'\n",
    "\n",
    "with open(folder_dataset, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Initialize variables for calculations\n",
    "total_instruction_length = 0\n",
    "instruction_count = 0\n",
    "task_difficulty_lengths = {1: [], 2: [], 3: []}\n",
    "canonical_code_lengths = {1: [], 2: [], 3: []}\n",
    "task_canonical_lengths = {}\n",
    "\n",
    "# Iterate through each item in the dataset\n",
    "for item in dataset:\n",
    "    task_id = item['TaskId']\n",
    "    instruction = item['Instruction']\n",
    "    difficulty = item['Difficulty']\n",
    "\n",
    "    # Skip if Difficulty is NaN\n",
    "    if math.isnan(difficulty):\n",
    "        continue\n",
    "\n",
    "    difficulty = int(difficulty)  # Convert difficulty to an integer\n",
    "\n",
    "    # Calculate the total instruction length and count\n",
    "    instruction_length = len(str(instruction))\n",
    "    total_instruction_length += instruction_length\n",
    "    instruction_count += 1\n",
    "\n",
    "    # Append the instruction length to the respective difficulty level list\n",
    "    task_difficulty_lengths[difficulty].append(instruction_length)\n",
    "\n",
    "    # Read the corresponding canonical code file\n",
    "    canonical_code_file = os.path.join(folder_canonical, f'{task_id}_CanonicalCode.py')\n",
    "    if os.path.exists(canonical_code_file):\n",
    "        with open(canonical_code_file, 'r') as code_file:\n",
    "            canonical_code = code_file.read()\n",
    "            canonical_code_length = len(canonical_code)\n",
    "            task_canonical_lengths[task_id] = canonical_code_length\n",
    "\n",
    "            # Append the canonical code length to the respective difficulty level list\n",
    "            canonical_code_lengths[difficulty].append(canonical_code_length)\n",
    "\n",
    "# Calculate average instruction length\n",
    "average_instruction_length = total_instruction_length / instruction_count\n",
    "\n",
    "# Calculate average lengths by difficulty, avoiding division by zero\n",
    "average_instruction_lengths_by_difficulty = {difficulty: (sum(lengths) / len(lengths)) if len(lengths) > 0 else 0\n",
    "                                             for difficulty, lengths in task_difficulty_lengths.items()}\n",
    "\n",
    "average_canonical_code_lengths_by_difficulty = {difficulty: (sum(lengths) / len(lengths)) if len(lengths) > 0 else 0\n",
    "                                                for difficulty, lengths in canonical_code_lengths.items()}\n",
    "\n",
    "# Calculate overall average canonical code length\n",
    "if len(task_canonical_lengths) > 0:\n",
    "    overall_average_canonical_code_length = sum(task_canonical_lengths.values()) / len(task_canonical_lengths)\n",
    "else:\n",
    "    overall_average_canonical_code_length = 0\n",
    "\n",
    "# Output the results\n",
    "print(f'Average Instruction Length: {average_instruction_length}')\n",
    "print(f'Average Instruction Length by Difficulty: {average_instruction_lengths_by_difficulty}')\n",
    "print(f'Overall Average Canonical Code Length: {overall_average_canonical_code_length}')\n",
    "print(f'Average Canonical Code Length by Difficulty: {average_canonical_code_lengths_by_difficulty}')\n",
    "\n",
    "# Print average instruction lengths by difficulty level\n",
    "for difficulty, avg_length in average_instruction_lengths_by_difficulty.items():\n",
    "    print(f'Average Instruction Length for Difficulty {difficulty}: {avg_length}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo-IMu3vKF7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
