{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCEVAL Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MatchEndpoints\n",
    "## DTW\n",
    "## Pass@1\n",
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing logs: 100%|██████████| 186/186 [00:31<00:00,  5.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from tslearn.metrics import dtw\n",
    "\n",
    "# Define folder paths and LLM name\n",
    "llm_name = 'o3-mini-M1' \n",
    "folder_canonical = f'/Users/yin/Documents/GitHub/MCCodeLog/CanonicalCode'\n",
    "folder_llm = f'/Users/yin/Documents/GitHub/MCCodeLog/{llm_name}'\n",
    "\n",
    "# Define task ID range\n",
    "task_id_range = range(1, 187)\n",
    "\n",
    "# Initialize result lists for summary\n",
    "match_endpoints_summary = []\n",
    "dtw_summary = []\n",
    "\n",
    "# Load difficulty levels from JSON\n",
    "with open(\"./docs/WMX3API_MCEval_Evaluation_Dataset.json\", 'r') as file:\n",
    "    difficulty_data = json.load(file)\n",
    "\n",
    "difficulty_levels = {task['TaskId']: task['Difficulty'] for task in difficulty_data}\n",
    "\n",
    "# Initialize counters for statistics\n",
    "difficulty_match_counts = {1: 0, 2: 0, 3: 0}\n",
    "difficulty_dtw_zero_counts = {1: 0, 2: 0, 3: 0}\n",
    "difficulty_totals = {1: 0, 2: 0, 3: 0}\n",
    "match_endpoints_and_dtw_zero_counts = {1: 0, 2: 0, 3: 0}\n",
    "total_tasks = len(task_id_range)\n",
    "match_endpoints_and_dtw_zero_total = 0\n",
    "\n",
    "# Update difficulty totals\n",
    "for task in difficulty_data:\n",
    "    difficulty = task['Difficulty']\n",
    "    if difficulty in difficulty_totals:\n",
    "        difficulty_totals[difficulty] += 1\n",
    "\n",
    "def compare_logs(task_id):\n",
    "    \"\"\"\n",
    "    Compare logs for a given task ID using MatchEndpoints and DTW.\n",
    "    \"\"\"\n",
    "    global total_tasks, match_endpoints_and_dtw_zero_total\n",
    "    canonical_file = f'{folder_canonical}/{task_id}_CanonicalCode_log.txt'\n",
    "    llm_file = f'{folder_llm}/{task_id}_{llm_name}_log.txt'\n",
    "    \n",
    "    # Check if both files exist\n",
    "    if not os.path.exists(canonical_file) or not os.path.exists(llm_file):\n",
    "        match_endpoints_summary.append(f'{task_id}: No log file')\n",
    "        dtw_summary.append(f'{task_id}: No log file')\n",
    "        return None\n",
    "\n",
    "    # Read the log files\n",
    "    canonical_log = pd.read_csv(canonical_file, delimiter='\\t')\n",
    "    llm_log = pd.read_csv(llm_file, delimiter='\\t')\n",
    "\n",
    "    # Compare MatchEndpoints\n",
    "    match_endpoints_result, match = compare_match_endpoints(canonical_log, llm_log, task_id)\n",
    "    match_endpoints_summary.append(match_endpoints_result)\n",
    "\n",
    "    # Compare DTW\n",
    "    dtw_result, dtw_zero = compare_dtw(canonical_log, llm_log, task_id)\n",
    "    dtw_summary.append(str(dtw_zero) + ' ' + dtw_result)\n",
    "\n",
    "    # Check if both conditions are met\n",
    "    if match and dtw_zero:\n",
    "        match_endpoints_and_dtw_zero_total += 1\n",
    "        difficulty = difficulty_levels.get(task_id, None)\n",
    "        if difficulty and difficulty in match_endpoints_and_dtw_zero_counts:\n",
    "            match_endpoints_and_dtw_zero_counts[difficulty] += 1\n",
    "\n",
    "    try:\n",
    "        # Plot DTW alignments\n",
    "        plot_dtw_alignment(canonical_log, llm_log, task_id)\n",
    "    except:pass\n",
    "\n",
    "\n",
    "def compare_match_endpoints(canonical_log, llm_log, task_id):\n",
    "    \"\"\"\n",
    "    Compare the last point of each data column in canonical and LLM logs.\n",
    "    \"\"\"\n",
    "    if list(canonical_log.columns) != list(llm_log.columns):\n",
    "        return f'{task_id}: Endpoints Error - Column names not same', False\n",
    "\n",
    "    match = True\n",
    "    for column in canonical_log.columns[1:]:\n",
    "        if '_0.0' in column:\n",
    "            continue\n",
    "        if canonical_log[column].iloc[-1] != llm_log[column].iloc[-1]:\n",
    "            match = False\n",
    "            break\n",
    "\n",
    "    # Update statistics for difficulty level\n",
    "    difficulty = difficulty_levels.get(task_id, None)\n",
    "    if difficulty and difficulty in difficulty_match_counts:\n",
    "        if match:\n",
    "            difficulty_match_counts[difficulty] += 1\n",
    "\n",
    "    return f'{task_id}: {\"Match\" if match else \"DifferentEndPoints\"}', match\n",
    "\n",
    "def compare_dtw(canonical_log, llm_log, task_id):\n",
    "    \"\"\"\n",
    "    Compare the DTW distance of each data column in canonical and LLM logs.\n",
    "    \"\"\"\n",
    "    if list(canonical_log.columns) != list(llm_log.columns):\n",
    "        return f'{task_id}: dtw Error - Column names not same', False\n",
    "\n",
    "    dtw_distances = {}\n",
    "    dtw_zero = True\n",
    "    for column in canonical_log.columns[1:]:\n",
    "        if '_0.0' in column:\n",
    "            continue\n",
    "        canonical_data = canonical_log[column].iloc[1:]\n",
    "        llm_data = llm_log[column].iloc[1:]\n",
    "        distance = dtw(canonical_data, llm_data)\n",
    "        dtw_distances[column] = distance\n",
    "        if distance != 0:\n",
    "            dtw_zero = False\n",
    "\n",
    "    # Update statistics for difficulty level\n",
    "    difficulty = difficulty_levels.get(task_id, None)\n",
    "    if difficulty and difficulty in difficulty_dtw_zero_counts:\n",
    "        if dtw_zero:\n",
    "            difficulty_dtw_zero_counts[difficulty] += 1\n",
    "\n",
    "    return f'{task_id}: {dtw_distances}', dtw_zero\n",
    "\n",
    "def plot_dtw_alignment(canonical_log, llm_log, task_id):\n",
    "    \"\"\"\n",
    "    Plot the DTW alignment for each data column in canonical and LLM logs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for column in canonical_log.columns[1:]:\n",
    "        if '_0.0' in column:\n",
    "            continue\n",
    "        canonical_data = canonical_log[column].iloc[1:].values\n",
    "        llm_data = llm_log[column].iloc[1:].values\n",
    "        sns.lineplot(x=range(len(canonical_data)), y=canonical_data, label=f'Canonical {column}', linestyle='--')\n",
    "        sns.lineplot(x=range(len(llm_data)), y=llm_data, label=f'{llm_name} {column}')\n",
    "    plt.title(f'DTW Alignment for Task ID {task_id}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{folder_llm}/{task_id}_DTW_Alignment.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def write_results_to_file():\n",
    "    \"\"\"\n",
    "    Write the results to MatchEndpoints.txt and DTW.txt in the LLM folder.\n",
    "    \"\"\"\n",
    "    with open(f'{folder_llm}/MatchEndpoints.txt', 'w') as file:\n",
    "        for result in match_endpoints_summary:\n",
    "            file.write(result + '\\n')\n",
    "        correct_matches = len([result for result in match_endpoints_summary if 'Match' in result])\n",
    "        total_matches = len(match_endpoints_summary)\n",
    "        file.write(f'Correct Matches: {correct_matches}/{total_matches}\\n')\n",
    "        file.write(f'Match Proportion: {correct_matches/total_matches:.2%}\\n')\n",
    "\n",
    "        for difficulty in difficulty_totals:\n",
    "            if difficulty_totals[difficulty] > 0:\n",
    "                match_ratio = difficulty_match_counts[difficulty] / difficulty_totals[difficulty]\n",
    "                file.write(f'Difficulty {difficulty} Match Proportion: {match_ratio:.2%} ({difficulty_match_counts[difficulty]}/{difficulty_totals[difficulty]})\\n')\n",
    "\n",
    "    with open(f'{folder_llm}/DTW.txt', 'w') as file:\n",
    "        for result in dtw_summary:\n",
    "            file.write(result + '\\n')\n",
    "        file.write(f'Total Comparisons: {len(dtw_summary)}\\n')\n",
    "\n",
    "        dtw_zero_counts = sum([1 for result in dtw_summary if 'True' in result])\n",
    "        file.write(f'DTW Zero Proportion: {dtw_zero_counts}/{len(dtw_summary)}: {dtw_zero_counts/len(dtw_summary):.2%}\\n')\n",
    "\n",
    "        for difficulty in difficulty_totals:\n",
    "            if difficulty_totals[difficulty] > 0:\n",
    "                dtw_zero_ratio = difficulty_dtw_zero_counts[difficulty] / difficulty_totals[difficulty]\n",
    "                file.write(f'Difficulty {difficulty} DTW Zero Proportion: {dtw_zero_ratio:.2%} ({difficulty_dtw_zero_counts[difficulty]}/{difficulty_totals[difficulty]})\\n')\n",
    "\n",
    "    with open(f'{folder_llm}/MatchEndpointsAndDTWZero.txt', 'w') as file:\n",
    "        file.write(f'Total Match Endpoints and DTW Zero: {match_endpoints_and_dtw_zero_total}/{total_tasks} ({match_endpoints_and_dtw_zero_total/total_tasks:.2%})\\n')\n",
    "        for difficulty in difficulty_totals:\n",
    "            if difficulty_totals[difficulty] > 0:\n",
    "                match_endpoints_and_dtw_zero_ratio = match_endpoints_and_dtw_zero_counts[difficulty] / difficulty_totals[difficulty]\n",
    "                file.write(f'Difficulty {difficulty} Match Endpoints and DTW Zero: {match_endpoints_and_dtw_zero_counts[difficulty]}/{difficulty_totals[difficulty]} ({match_endpoints_and_dtw_zero_ratio:.2%})\\n')\n",
    "\n",
    "# Compare logs for each task_id with progress display\n",
    "for task_id in tqdm(task_id_range, desc=\"Comparing logs\"):\n",
    "    compare_logs(task_id)\n",
    "\n",
    "# Write the results to the respective files\n",
    "write_results_to_file()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
